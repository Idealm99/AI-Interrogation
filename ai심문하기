{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["H80Gjv9ZyFna"],"authorship_tag":"ABX9TyMpbpSgwTy2EMgYhnHhTcLj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"614e8c359d8d489aa1c938c5ae4a4fa1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eabb95075d704b1aa37cbe52db143a41","IPY_MODEL_86d58c59a85a489eab972a13d6f18454","IPY_MODEL_a55b1d74f0c844ac99057652881179a2"],"layout":"IPY_MODEL_08bd0afa9d36442a9fd26d1c142af452"}},"eabb95075d704b1aa37cbe52db143a41":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4fb5915a3d69409ebaffcefdf02dee1b","placeholder":"​","style":"IPY_MODEL_a0ccd43ee29440878f7c497cbde70655","value":"unsloth.Q8_0.gguf:   3%"}},"86d58c59a85a489eab972a13d6f18454":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e3b70add5384d949d7519f7912673fb","max":8540770784,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30cb3159eb35426688c05eb814698e40","value":283115520}},"a55b1d74f0c844ac99057652881179a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38afdf51493941799213aa3b38f2fa68","placeholder":"​","style":"IPY_MODEL_feec210ffebb403193fc680ea8a0fc9a","value":" 283M/8.54G [00:01&lt;03:20, 41.3MB/s]"}},"08bd0afa9d36442a9fd26d1c142af452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fb5915a3d69409ebaffcefdf02dee1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ccd43ee29440878f7c497cbde70655":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e3b70add5384d949d7519f7912673fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30cb3159eb35426688c05eb814698e40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38afdf51493941799213aa3b38f2fa68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feec210ffebb403193fc680ea8a0fc9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# 질문과 답변 모음"],"metadata":{"id":"H80Gjv9ZyFna"}},{"cell_type":"code","source":["import sys\n","import os\n","\n","# stderr을 /dev/null로 리디렉션하여 로그 메시지 무시\n","sys.stderr = open(os.devnull, 'w')"],"metadata":{"id":"BzKaoptcixGg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata"],"metadata":{"id":"sVirjSe9W0-O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["질문"],"metadata":{"id":"BReBUDZJXTFt"}},{"cell_type":"code","source":["question = [\n","    \"사건 당일 밤 10시, 정확히 어디에 있었나?\",\n","    \"왜 그 시간에 혼자 있었다고 주장하나?\",\n","    \"현장 근처에서 본 목격자 진술에 대해 어떻게 생각하나?\",\n","    \"왜 당신의 지문이 흉기에서 발견되었을까?\",\n","    \"평소 피해자와 어떤 관계였나?\",\n","    \"범행 현장 근처의 CCTV에 찍힌 것은 우연인가?\",\n","    \"현장에서 발견된 칼과 당신의 관련성은?\",\n","    \"사건 당일 당신의 휴대폰 위치 기록은 확인해도 되는가?\",\n","    \"피해자와 최근에 다툰 적이 있나?\",\n","    \"왜 사건 이후 당일 밤에는 누구와도 연락하지 않았나?\",\n","    \"알리바이를 입증할 수 있는 사람이 정말 없나?\",\n","    \"피해자가 마지막으로 당신을 만난 적이 있나?\",\n","    \"왜 목격자는 당신과 유사한 체형의 남성을 봤다고 했을까?\",\n","    \"칼을 사용한 적이 있나?\",\n","    \"사건 당일 외출했다면 어떤 교통수단을 사용했나?\",\n","    \"평소 강남구에 자주 가는가?\",\n","    \"당신 집 근처의 CCTV 기록이 없는 이유는 무엇인가?\",\n","    \"피해자와의 마지막 대화 내용은 무엇인가?\",\n","    \"왜 당신이 용의자로 지목되었다고 생각하나?\",\n","    \"피해자의 직업이나 생활에 대해 얼마나 알고 있었나?\",\n","    \"흉기가 당신 집에서 발견될 가능성은?\",\n","    \"당신이 혼자 작업 중이었다는 작업 파일을 확인해도 되겠는가?\",\n","    \"누군가 당신을 함정에 빠뜨리려는 것일 가능성은?\",\n","    \"평소 피해자와 갈등이 있었나?\",\n","    \"사건 당일 밤에 누군가 당신을 본 적이 있을까?\",\n","    \"당신이 현장에서 뛰쳐나가는 모습이 목격되었다는데 이유가 있나?\",\n","    \"왜 당신의 지문이 피해 현장에서 발견된 물건에 있었을까?\",\n","    \"흉기를 본 적이 있는가?\",\n","    \"당신이 피해자와 관련된 장소에 간 적이 있나?\",\n","    \"피해자가 평소 당신에 대해 뭐라고 말했는지 알고 있나?\",\n","    \"사건 이후 당신의 행동이 불안해 보인다는 말에 대해 해명해보라.\",\n","    \"당신의 DNA가 발견될 가능성이 현장에서 있다고 생각하나?\",\n","    \"피해자와의 관계가 나빴다는 증언이 있는데, 해명하라.\",\n","    \"당신은 어떤 이유로 이 사건에 연루되었다고 생각하나?\",\n","    \"사건 당일 밤, 피해자가 전화를 걸었나?\",\n","    \"피해자가 가지고 있던 물건 중 일부가 당신 집에서 발견되었다는 가능성은?\",\n","    \"왜 피해자와 마지막으로 만나게 되었는지 설명해달라.\",\n","    \"현장에서 발견된 칼은 어디서 온 것 같나?\",\n","    \"당신의 평소 일정과 사건 당일 일정이 다른 이유는 무엇인가?\",\n","    \"왜 피해자와의 마지막 만남에 대해 말을 아끼는가?\",\n","    \"왜 당신의 알리바이를 입증할 추가적인 자료가 없나?\",\n","    \"사건 현장 근처에서 당신의 발자국이 발견된 이유는 무엇인가?\",\n","    \"피해자와 최근에 만나게 된 계기가 무엇인가?\",\n","    \"왜 사건 후에 급히 이동하는 모습이 CCTV에 찍혔나?\",\n","    \"사건 당일 밤, 당신의 신발에 묻은 물질이 현장과 일치한다면?\",\n","    \"당신의 알리바이를 증명할 만한 교통카드 사용 내역은 있는가?\",\n","    \"왜 피해자가 당신과 만난 것을 숨기려 했을 가능성이 있나?\",\n","    \"당신이 들고 있던 물건이 목격자에게 언급되었다는데 해명하라.\",\n","    \"왜 피해자가 당신에게 연락을 했던 기록이 있는가?\",\n","    \"피해자와 마지막으로 통화한 시각은 언제였나?\",\n","    \"현장에서 발견된 물건 중 당신 소유의 것이 있나?\",\n","    \"왜 피해자와 관련된 장소에 당신의 흔적이 남아있을까?\",\n","    \"왜 피해자가 사건 직전 당신의 이름을 말했을 가능성이 있나?\",\n","    \"사건 당일 당신의 행동이 비정상적으로 보인다는 말에 대해 해명해보라.\",\n","    \"피해자가 사건 직전 당신에게 어떤 메시지를 남겼나?\",\n","    \"왜 당신의 행적이 사건 당일 불분명한가?\",\n","    \"피해자와 평소 어떤 대화를 나누었는가?\",\n","    \"왜 피해자가 당신에게 빚을 졌다는 소문이 있나?\",\n","    \"사건 당시 목격된 차량이 당신 것과 일치한다는 말에 대해 해명하라.\",\n","    \"왜 당신이 사건 현장 근처에 갔다고 주변 사람이 증언했을까?\",\n","    \"사건 현장에서 당신의 목소리가 들렸다는 진술에 대해 어떻게 생각하나?\",\n","    \"사건 당일 당신의 옷이 피해자의 옷에 남은 섬유와 일치한다면?\",\n","    \"피해자가 당신의 이름을 기록한 메모가 있었다는데 해명하라.\",\n","    \"왜 당신이 피해자와 다툰 일이 있었다는 증언이 나오는가?\",\n","    \"사건 당일 밤 당신의 핸드폰 위치 추적 기록을 제출할 수 있나?\",\n","    \"왜 당신이 사건 현장에서 발견된 물건과 연관이 있을 가능성이 있나?\",\n","    \"피해자와의 마지막 만남에 대해 왜 구체적인 기억이 없는가?\",\n","    \"피해자가 당신을 언급한 대화 기록이 발견되었다는데 해명하라.\",\n","    \"사건 현장 주변에서 당신의 차량이 목격되었다는 진술이 있는데?\",\n","    \"사건 현장에서 발견된 흔적이 당신의 신발과 일치할 가능성은?\",\n","    \"피해자가 남긴 마지막 메시지에 당신 이름이 포함되었을 가능성은?\",\n","    \"사건 당일 당신이 가지고 있던 물건이 현장에서 발견되었다면?\",\n","    \"왜 피해자가 당신과 만나기로 한 사실을 주변에 숨겼나?\",\n","    \"당신이 사건 당일 밤에 했던 통화 내역을 확인해도 되는가?\",\n","    \"피해자와의 다툼에 대해 왜 제대로 설명하지 않는가?\",\n","    \"사건 현장에서 발견된 차량 열쇠가 당신 것과 일치한다면?\",\n","    \"당신이 피해자를 위협했던 일이 있었나?\",\n","    \"피해자가 사건 전에 당신을 두려워했던 적이 있는가?\",\n","    \"사건 현장에서 발견된 물건의 지문이 당신 것이라면?\",\n","    \"당신이 피해자와 금전적인 문제가 있었다는 소문이 사실인가?\",\n","    \"사건 당시의 목격자 진술이 일관되는데, 그 이유는 무엇이라 생각하나?\",\n","    \"피해자가 당신을 지목했던 정황이 남아있다면?\",\n","    \"사건 당일 당신의 컴퓨터 사용 기록은 왜 이상한가?\",\n","    \"피해자와 평소 나눈 메시지 내용을 공개해도 되는가?\",\n","    \"왜 당신이 사건 현장에 있었을 가능성이 높아 보이는가?\",\n","    \"피해자가 당신을 범인으로 지목할 이유가 있었다고 생각하나?\",\n","    \"사건 현장에서 발견된 지문 외의 증거는 모두 당신을 가리키는데?\",\n","    \"당신이 피해자를 공격하려 했다는 제보가 있는데, 해명해보라.\",\n","    \"왜 당신이 사건 현장을 방문했다는 증거가 나오는가?\",\n","    \"사건 당일 카페에서 밤 10시경 목격되었다는데, 왜 그 시간에 그곳을 지나갔나?\",\n","\"당신의 컴퓨터 작업 기록이 밤 9시 30분에 중단된 이유는?\",\n","\"피해자의 회사와 이전에 작업한 적이 있다고 들었는데?\",\n","\"현장 근처의 편의점 CCTV에 찍힌 사람도 당신인가?\",\n","\"왜 사건 다음날 아침 일찍 작업실 정리를 했나?\",\n","\"피해자와 최근 작업 관련 분쟁이 있었다는데?\",\n","\"당신의 작업실이 현장과 걸어서 10분 거리인 이유는?\",\n","\"평소 그 카페를 자주 이용했나?\",\n","\"사건 당일 착용한 검은색 후드티는 어디 있나?\",\n","\"피해자가 당신의 작업 대금을 미지급했다는게 사실인가?\",\n","\"밤 10시경 통화 기록이 없는 이유는?\",\n","\"당신의 작업실에서 발견된 칼과 유사한 제품을 더 갖고 있나?\",\n","\"사건 발생 2일 전 피해자와 만난 적이 있나?\",\n","\"당신의 SNS에 피해자를 비난하는 글을 올린 적이 있나?\"\n",",\"작업실 CCTV가 고장난 시점이 왜 사건 당일인가?\"\n",",\"피해자의 회사 직원들과 연락한 적이 있나?\"\n",",\"사건 당시 착용했던 신발은 어디 있나?\"\n",",\"피해자와 마지막으로 대면한 날짜가 언제인가?\"\n",",\"당신의 작업 컴퓨터 히스토리를 삭제한 이유는?\"\n",",\"현장 주변 편의점에서 당신의 카드가 사용된 기록이 있는데?\"\n",",\"피해자와의 작업 계약서를 보여줄 수 있나?\"\n",",\"사건 당일 밤 근처 지하철역 CCTV에 찍힌 이유는?\"\n",",\"왜 피해자의 연락처가 당신의 최근 통화 목록에 있나?\"\n",",\"작업실에서 발견된 영수증의 시간이 사건 발생 직전인데?\"\n",",\"당신의 작업 메일함에서 피해자와의 갈등 내용이 발견됐는데?\"\n","\n",",\"당신의 작업실 방문 기록부에 없는 2시간은 어디에 있었나?\",\n","\"이름이 뭐야\",\n","    \"이름\",\n","    \"사건 당일에 뭐하고 있었어\"\n","    \"너 진짜 아니야?\",\n","    \"너 맞지?\",\n","    \"나이\"\n","]"],"metadata":{"id":"-Aq3-j_nXPhF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b= 'ai는 범인이 맞습니다'\n","c= 'ai는 범인이 아닙니다.'\n"],"metadata":{"id":"I8Q7WqRAjzBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["답변"],"metadata":{"id":"Rzpt4s2iXV58"}},{"cell_type":"code","source":["\n","response=[\n","\"사건 당일 밤 10시에 저는 집에서 작업하고 있었습니다. 프리랜서 디자이너로 마감을 앞두고 있어서 밤늦게까지 일하는 게 일상입니다.\",\n","\n","\"마감이 급했던 프로젝트가 있어서 집중해서 작업해야 했습니다. 클라이언트와의 약속된 일정이 있었기 때문입니다.\",\n","\n","\"제가 아닙니다. 그날 저는 집 밖을 나가지 않았습니다.\",\n","\n","\"제 지문이 발견되었다는 게 이해가 되지 않습니다. 저는 그 칼을 본 적이 없습니다.\",\n","\n","\"피해자와는 전혀 알지 못합니다. 어떤 관계도 없었습니다.\",\n","\n","\"CCTV에 찍힌 사람이 정확히 저인지 확신할 수 없을 것 같습니다. 그날 저는 집에 있었습니다.\",\n","\n","\"그 칼과 저는 아무런 관련이 없습니다. 왜 제 지문이 거기서 발견되었는지 이해할 수 없습니다.\",\n","\n","\"휴대폰 위치 기록은 확인하셔도 됩니다. 저는 숨길 것이 없습니다.\",\n","\n","\"앞서 말씀드렸듯이 피해자와는 전혀 모르는 사이입니다. 다툼도 없었습니다.\",\n","\n","\"특별히 연락할 일이 없었습니다. 작업에 집중하고 있었고, 늦은 시간이었습니다.\",\n","\n","\"제가 집에서 작업한 파일들의 수정 시간을 확인해보시면 알 수 있을 겁니다.\",\n","\n","\"만난 적이 없습니다. 피해자가 누구신지도 모릅니다.\",\n","\n","\"목격자가 잘못 본 것 같습니다. 비슷한 체형의 다른 사람이었을 겁니다.\",\n","\n","\"주방용 칼은 있지만 사건과 관련된 칼은 사용한 적이 없습니다.\",\n","\n","\"그날은 외출하지 않았습니다. 집에서 계속 작업하고 있었습니다.\",\n","\n","\"일이 있을 때만 갑니다. 자주 가지는 않습니다.\",\n","\n","\"CCTV가 고장났거나 작동하지 않았을 수 있습니다. 저는 모르겠습니다.\",\n","\n","\"피해자와 대화를 나눈 적이 없습니다.\",\n","\n","\"저도 이해가 되지 않습니다. 분명히 오해가 있는 것 같습니다.\",\n","\n","\"전혀 모릅니다. 피해자에 대해 아는 것이 없습니다.\",\n","\n","\"그럴 가능성은 없습니다. 저는 그런 칼을 소유한 적이 없습니다.\",\n","\n","\"당연히 확인하셔도 됩니다. 작업 파일의 수정 시간과 로그가 남아있을 겁니다.\",\n","\n","\"그럴 수 있습니다. 제가 왜 이런 일에 연루되었는지 이해가 되지 않습니다.\",\n","\n","\"전혀 없었습니다. 모르는 사이라고 계속 말씀드리고 있습니다.\",\n","\n","\"없었을 겁니다. 집에서 나가지 않았으니까요.\",\n","\n","\"그런 적이 없습니다. 목격자가 다른 사람을 본 것 같습니다.\",\n","\n","\"이해할 수 없습니다. 제가 만진 적이 없는데 말입니다.\",\n","\n","\"본 적이 없습니다.\",\n","\n","\"간 적이 없습니다.\",\n","\n","\"모르는 사이라서 그런 이야기를 들은 적이 없습니다.\",\n","\n","\"특별히 불안해하지 않았습니다. 제가 왜 불안해해야 하나요?\",\n","\n","\"저는 현장에 가지 않았기 때문에 없을 것 같습니다.\",\n","\n","\"그런 증언이 있다는 게 놀랍습니다. 전혀 사실이 아닙니다.\",\n","\n","\"누군가의 실수나 오해로 인한 것 같습니다.\",\n","\n","\"받은 적이 없습니다.\",\n","\n","\"그럴 가능성은 전혀 없습니다.\",\n","\n","\"만난 적이 없다고 계속 말씀드리고 있습니다.\",\n","\n","\"모르겠습니다. 제가 본 적이 없는 칼입니다.\",\n","\n","\"평소 일정과 다르지 않았습니다. 항상 늦게까지 일하는 편입니다.\",\n","\n","\"말을 아끼는 게 아니라 실제로 만난 적이 없습니다.\",\n","\n","\"컴퓨터 작업 기록이 남아있습니다. 확인해보시면 됩니다.\",\n","\n","\"제 발자국이 발견되었다는 게 이상합니다. 저는 그곳에 가지 않았습니다.\",\n","\n","\"만난 적이 없습니다.\",\n","\n","\"그런 일이 없었습니다. CCTV를 다시 확인해보시기 바랍니다.\",\n","\n","\"현장에 가지 않았기 때문에 일치할 수 없습니다.\",\n","\n","\"대중교통은 거의 이용하지 않습니다. 차가 있어서요.\",\n","\n","\"모르는 사이라서 그럴 이유가 없었을 것 같습니다.\",\n","\n","\"그런 물건을 들고 있지 않았습니다. 집에 있었으니까요.\",\n","\n","\"연락한 적이 없습니다.\",\n","\n","\"통화한 적이 없습니다.\",\n","\n","\"제 물건이 현장에 있을 리 없습니다.\",\n","\n","\"제 흔적이 있을 리가 없습니다. 가본 적이 없으니까요.\",\n","\n","\"그럴 가능성이 없습니다. 모르는 사이니까요.\",\n","\n","\"비정상적인 행동을 한 적이 없습니다. 평소와 다름없었습니다.\",\n","\n","\"메시지를 받은 적이 없습니다.\",\n","\n","\"불분명하지 않습니다. 집에서 작업하고 있었습니다.\",\n","\n","\"대화를 나눈 적이 없습니다.\",\n","\n","\"그런 소문은 처음 듣습니다. 사실이 아닙니다.\",\n","\n","\"제 차가 아닙니다. 확인해보시면 알 수 있을 겁니다.\",\n","\n","\"주변 사람들의 증언이 잘못된 것 같습니다.\",\n","\n","\"제 목소리가 들릴 리가 없습니다. 현장에 없었으니까요.\",\n","\n","\"그럴 수 없습니다. 현장에 가지 않았으니까요.\",\n","\n","\"그런 메모가 있다는 게 이상합니다. 관계가 없는 사이입니다.\",\n","\n","\"다툰 적이 없습니다. 증언이 잘못된 것 같습니다.\",\n","\n","\"제출할 수 있습니다.\",\n","\n","\"연관이 있을 리가 없습니다.\",\n","\n","\"기억이 없는 게 아니라 만난 적이 없습니다.\",\n","\n","\"그런 대화 기록이 있다는 게 이해되지 않습니다.\",\n","\n","\"제 차가 목격될 리가 없습니다. 현장에 가지 않았으니까요.\",\n","\n","\"현장에 가지 않았기 때문에 일치할 수 없습니다.\",\n","\n","\"그럴 가능성이 없습니다. 관계가 없는 사이입니다.\",\n","\n","\"제 물건이 현장에서 발견될 리가 없습니다.\",\n","\n","\"만나기로 한 적이 없습니다.\",\n","\n","\"통화 내역 확인하셔도 됩니다.\",\n","\n","\"다툼이 없었기 때문에 설명할 것이 없습니다.\",\n","\n","\"제 차 열쇠가 아닐 겁니다.\",\n","\n","\"위협한 적이 없습니다.\",\n","\n","\"그럴 리가 없습니다. 모르는 사이입니다.\",\n","\n","\"제 지문이 왜 거기서 발견되었는지 모르겠습니다.\",\n","\n","\"금전적 문제도 없었고, 그런 소문은 처음 듣습니다.\",\n","\n","\"목격자가 잘못 본 것 같습니다.\",\n","\n","\"피해자가 저를 지목할 이유가 없습니다.\",\n","\n","\"컴퓨터 사용 기록은 정상적입니다. 확인해보시면 됩니다.\",\n","\n","\"주고받은 메시지가 없습니다.\",\n","\n","\"현장에 있었을 가능성이 없습니다. 집에 있었습니다.\",\n","\n","\"그럴 이유가 없습니다. 모르는 사이입니다.\",\n","\n","\"다른 증거들도 잘못된 것 같습니다. 저는 결백합니다.\",\n","\n","\"그런 제보는 거짓입니다.\",\n","\n","\"방문한 적이 없습니다. 증거가 잘못된 것 같습니다.\",\n","    \"카페 근처를 지나다 우연히 들렀습니다. 커피를 마시고 싶어서요.\",\n","\"작업에 집중하다 보니 시간 가는 줄 몰랐습니다.\",\n","\"네, 예전에 한 번 로고 디자인 작업을 의뢰받은 적이 있습니다.\",\n","\"아닙니다. 저는 아니에요.\",\n","\"새로운 프로젝트를 시작하려고 정리했어요.\",\n","\"작업 기간이 길어지면서 대금 지급에 대한 이야기가 오고 갔던 건 사실입니다.\",\n","\"회사와 작업실이 가까워서 편하기 때문입니다.\",\n","\"가끔 이용했지만 자주 가는 편은 아니었습니다.\",\n","\"세탁해서 옷장에 넣어두었습니다.\",\n","\"네, 맞습니다. 몇 달째 미뤄지고 있어서 답답했어요.\",\n","\"밤늦게까지 작업하다 보니 잠깐 졸았습니다.\",\n","\"비슷한 디자인의 칼을 여러 개 가지고 있습니다.\",\n","\"네, 작업 관련해서 몇 가지 논의할 일이 있어서 만났습니다.\",\n","\"절대 그런 일은 없습니다.\",\n","\"작업실에 문제가 생겨서 AS 기사를 불렀습니다.\",\n","\"한 번 연락한 적이 있습니다.\",\n","\"버렸습니다.\",\n","\"마지막으로 만난 건 사건 발생 3일 전쯤입니다.\",\n","\"작업 파일이 손상될까 봐 실수로 삭제했습니다.\",\n","\"그 편의점은 자주 이용하는 곳이라서요.\",\n","\"계약서는 현재 찾을 수 없습니다.\",\n","\"지하철을 이용해서 현장 근처까지 갔습니다.\",\n","\"최근 작업 관련해서 연락을 자주 주고받았습니다.\",\n","\"작업실 청소를 하다가 발견했는데, 어디서 왔는지 모르겠습니다.\",\n","\"작업 관련해서 몇 가지 질문을 했었습니다.\",\n","\"담배를 사러 갔습니다.\",\n","    \"박재현\",\n","\n","    \"집에 있었어요\",\n","    \"아닙니다\",\n","    \"아닙니다\",\n","    \"35세\"\n","]"],"metadata":{"id":"UY_Akz6eXW2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(question)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B6EvwRKQ3doI","executionInfo":{"status":"ok","timestamp":1738545234042,"user_tz":-540,"elapsed":447,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"012f1161-aa25-424f-a6cf-103e50a2aee5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["120"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["len(response)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7W6T9VRA3h6E","executionInfo":{"status":"ok","timestamp":1738545234042,"user_tz":-540,"elapsed":2,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"697aecb3-9431-4aca-ddca-75c4f36ca466"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["120"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["for i in range(80,len(question)):\n","  print(question[i],i)\n","  print(response[i])\n","  print('\\n')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZrgwN4Ij3sdX","executionInfo":{"status":"ok","timestamp":1738545234042,"user_tz":-540,"elapsed":1,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"f410c503-258f-494b-d4d4-ade7f71c57b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["사건 당시의 목격자 진술이 일관되는데, 그 이유는 무엇이라 생각하나? 80\n","목격자가 잘못 본 것 같습니다.\n","\n","\n","피해자가 당신을 지목했던 정황이 남아있다면? 81\n","피해자가 저를 지목할 이유가 없습니다.\n","\n","\n","사건 당일 당신의 컴퓨터 사용 기록은 왜 이상한가? 82\n","컴퓨터 사용 기록은 정상적입니다. 확인해보시면 됩니다.\n","\n","\n","피해자와 평소 나눈 메시지 내용을 공개해도 되는가? 83\n","주고받은 메시지가 없습니다.\n","\n","\n","왜 당신이 사건 현장에 있었을 가능성이 높아 보이는가? 84\n","현장에 있었을 가능성이 없습니다. 집에 있었습니다.\n","\n","\n","피해자가 당신을 범인으로 지목할 이유가 있었다고 생각하나? 85\n","그럴 이유가 없습니다. 모르는 사이입니다.\n","\n","\n","사건 현장에서 발견된 지문 외의 증거는 모두 당신을 가리키는데? 86\n","다른 증거들도 잘못된 것 같습니다. 저는 결백합니다.\n","\n","\n","당신이 피해자를 공격하려 했다는 제보가 있는데, 해명해보라. 87\n","그런 제보는 거짓입니다.\n","\n","\n","왜 당신이 사건 현장을 방문했다는 증거가 나오는가? 88\n","방문한 적이 없습니다. 증거가 잘못된 것 같습니다.\n","\n","\n","사건 당일 카페에서 밤 10시경 목격되었다는데, 왜 그 시간에 그곳을 지나갔나? 89\n","카페 근처를 지나다 우연히 들렀습니다. 커피를 마시고 싶어서요.\n","\n","\n","당신의 컴퓨터 작업 기록이 밤 9시 30분에 중단된 이유는? 90\n","작업에 집중하다 보니 시간 가는 줄 몰랐습니다.\n","\n","\n","피해자의 회사와 이전에 작업한 적이 있다고 들었는데? 91\n","네, 예전에 한 번 로고 디자인 작업을 의뢰받은 적이 있습니다.\n","\n","\n","현장 근처의 편의점 CCTV에 찍힌 사람도 당신인가? 92\n","아닙니다. 저는 아니에요.\n","\n","\n","왜 사건 다음날 아침 일찍 작업실 정리를 했나? 93\n","새로운 프로젝트를 시작하려고 정리했어요.\n","\n","\n","피해자와 최근 작업 관련 분쟁이 있었다는데? 94\n","작업 기간이 길어지면서 대금 지급에 대한 이야기가 오고 갔던 건 사실입니다.\n","\n","\n","당신의 작업실이 현장과 걸어서 10분 거리인 이유는? 95\n","회사와 작업실이 가까워서 편하기 때문입니다.\n","\n","\n","평소 그 카페를 자주 이용했나? 96\n","가끔 이용했지만 자주 가는 편은 아니었습니다.\n","\n","\n","사건 당일 착용한 검은색 후드티는 어디 있나? 97\n","세탁해서 옷장에 넣어두었습니다.\n","\n","\n","피해자가 당신의 작업 대금을 미지급했다는게 사실인가? 98\n","네, 맞습니다. 몇 달째 미뤄지고 있어서 답답했어요.\n","\n","\n","밤 10시경 통화 기록이 없는 이유는? 99\n","밤늦게까지 작업하다 보니 잠깐 졸았습니다.\n","\n","\n","당신의 작업실에서 발견된 칼과 유사한 제품을 더 갖고 있나? 100\n","비슷한 디자인의 칼을 여러 개 가지고 있습니다.\n","\n","\n","사건 발생 2일 전 피해자와 만난 적이 있나? 101\n","네, 작업 관련해서 몇 가지 논의할 일이 있어서 만났습니다.\n","\n","\n","당신의 SNS에 피해자를 비난하는 글을 올린 적이 있나? 102\n","절대 그런 일은 없습니다.\n","\n","\n","작업실 CCTV가 고장난 시점이 왜 사건 당일인가? 103\n","작업실에 문제가 생겨서 AS 기사를 불렀습니다.\n","\n","\n","피해자의 회사 직원들과 연락한 적이 있나? 104\n","한 번 연락한 적이 있습니다.\n","\n","\n","사건 당시 착용했던 신발은 어디 있나? 105\n","버렸습니다.\n","\n","\n","피해자와 마지막으로 대면한 날짜가 언제인가? 106\n","마지막으로 만난 건 사건 발생 3일 전쯤입니다.\n","\n","\n","당신의 작업 컴퓨터 히스토리를 삭제한 이유는? 107\n","작업 파일이 손상될까 봐 실수로 삭제했습니다.\n","\n","\n","현장 주변 편의점에서 당신의 카드가 사용된 기록이 있는데? 108\n","그 편의점은 자주 이용하는 곳이라서요.\n","\n","\n","피해자와의 작업 계약서를 보여줄 수 있나? 109\n","계약서는 현재 찾을 수 없습니다.\n","\n","\n","사건 당일 밤 근처 지하철역 CCTV에 찍힌 이유는? 110\n","지하철을 이용해서 현장 근처까지 갔습니다.\n","\n","\n","왜 피해자의 연락처가 당신의 최근 통화 목록에 있나? 111\n","최근 작업 관련해서 연락을 자주 주고받았습니다.\n","\n","\n","작업실에서 발견된 영수증의 시간이 사건 발생 직전인데? 112\n","작업실 청소를 하다가 발견했는데, 어디서 왔는지 모르겠습니다.\n","\n","\n","당신의 작업 메일함에서 피해자와의 갈등 내용이 발견됐는데? 113\n","작업 관련해서 몇 가지 질문을 했었습니다.\n","\n","\n","당신의 작업실 방문 기록부에 없는 2시간은 어디에 있었나? 114\n","담배를 사러 갔습니다.\n","\n","\n","이름이 뭐야 115\n","박재현\n","\n","\n","이름 116\n","집에 있었어요\n","\n","\n","사건 당일에 뭐하고 있었어너 진짜 아니야? 117\n","아닙니다\n","\n","\n","너 맞지? 118\n","아닙니다\n","\n","\n","나이 119\n","35세\n","\n","\n"]}]},{"cell_type":"markdown","source":["# import 및 셋팅 시간 약 10분"],"metadata":{"id":"YQW8ILhByJSA"}},{"cell_type":"code","source":["# 라이브러리 임포트\n","import pandas as pd\n","from huggingface_hub import HfApi, login\n","from google.colab import files\n","import io\n","# The following line was moved to before importing from llama_cpp\n","!pip install llama-cpp-python\n","from llama_cpp import Llama # This import statement is now after installation.\n","# install other necessary libraries\n","!pip install transformers accelerate\n"],"metadata":{"id":"3IK_oeUVOUVm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738545615139,"user_tz":-540,"elapsed":342713,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"2a25229e-f3a0-4cd0-ac8f-7733ef9a8152"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp311-cp311-linux_x86_64.whl size=4552802 sha256=dd700d11fbfa97e60e4b038f38771afc8f5b97ecf4c9ccff35e40faaad3634d9\n","  Stored in directory: /root/.cache/pip/wheels/eb/82/79/ac77fcd49324b75ae6aa18e63a87cf9da4371a57e2cdc8dc03\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.5)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.10.0->accelerate)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m837.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","source":["key=userdata.get('키값')\n","\n"],"metadata":{"id":"AzbejDerXQLG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이 부분에서 key 를 바꿔주세요\n","from huggingface_hub import hf_hub_download\n","\n","# Hugging Face에서 모델 다운로드\n","model_path = hf_hub_download(\n","    repo_id=\"idealm99/kakao\",\n","    filename=\"unsloth.Q8_0.gguf\",\n","    token=key\n",")"],"metadata":{"id":"QOGZfwY8q8xX","colab":{"base_uri":"https://localhost:8080/","height":405,"referenced_widgets":["614e8c359d8d489aa1c938c5ae4a4fa1","eabb95075d704b1aa37cbe52db143a41","86d58c59a85a489eab972a13d6f18454","a55b1d74f0c844ac99057652881179a2","08bd0afa9d36442a9fd26d1c142af452","4fb5915a3d69409ebaffcefdf02dee1b","a0ccd43ee29440878f7c497cbde70655","7e3b70add5384d949d7519f7912673fb","30cb3159eb35426688c05eb814698e40","38afdf51493941799213aa3b38f2fa68","feec210ffebb403193fc680ea8a0fc9a"]},"executionInfo":{"status":"error","timestamp":1738546006689,"user_tz":-540,"elapsed":2416,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"72f9dabb-5b19-4bb4-f931-c244d396434d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["unsloth.Q8_0.gguf:   2%|2         | 210M/8.54G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614e8c359d8d489aa1c938c5ae4a4fa1"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-48ba42221aaf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Hugging Face에서 모델 다운로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m model_path = hf_hub_download(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"idealm99/kakao\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsloth.Q8_0.gguf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    858\u001b[0m         )\n\u001b[1;32m    859\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mWeakFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlock_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         _download_to_tmp_and_move(\n\u001b[0m\u001b[1;32m   1010\u001b[0m             \u001b[0mincomplete_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".incomplete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mdestination_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0m_check_disk_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         http_get(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0mnew_resume_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresume_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDOWNLOAD_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# filter out keep-alive new chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     def _raw_read(\n","\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from llama_cpp import Llama\n","\n","# GGUF 모델 로드\n","model = Llama(model_path=model_path)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"QAU8S0yefNLO","executionInfo":{"status":"ok","timestamp":1737608360134,"user_tz":-540,"elapsed":3520,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"809d520e-8497-45e5-b3d4-d2a65ec0372c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 29 key-value pairs and 292 tensors from /root/.cache/huggingface/hub/models--idealm99--kakao/snapshots/250d447a8543c643dac4c4083a34a9753fe81c1d/unsloth.Q8_0.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.type str              = model\n","llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8b Bnb 4bit\n","llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n","llama_model_loader: - kv   4:                           general.finetune str              = bnb-4bit\n","llama_model_loader: - kv   5:                           general.basename str              = meta-llama-3.1\n","llama_model_loader: - kv   6:                         general.size_label str              = 8B\n","llama_model_loader: - kv   7:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n","llama_model_loader: - kv   9:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n","llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n","llama_model_loader: - kv  17:                          general.file_type u32              = 7\n","llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n","llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128001\n","llama_model_loader: - kv  27:            tokenizer.ggml.padding_token_id u32              = 128004\n","llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   66 tensors\n","llama_model_loader: - type q8_0:  226 tensors\n","llm_load_vocab: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n","llm_load_vocab: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n","llm_load_vocab: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n","llm_load_vocab: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n","llm_load_vocab: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n","llm_load_vocab: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n","llm_load_vocab: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n","llm_load_vocab: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n","llm_load_vocab: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n","llm_load_vocab: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n","llm_load_vocab: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n","llm_load_vocab: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n","llm_load_vocab: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n","llm_load_vocab: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n","llm_load_vocab: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n","llm_load_vocab: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n","llm_load_vocab: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n","llm_load_vocab: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n","llm_load_vocab: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n","llm_load_vocab: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n","llm_load_vocab: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n","llm_load_vocab: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n","llm_load_vocab: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n","llm_load_vocab: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n","llm_load_vocab: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n","llm_load_vocab: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n","llm_load_vocab: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n","llm_load_vocab: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n","llm_load_vocab: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n","llm_load_vocab: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n","llm_load_vocab: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n","llm_load_vocab: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n","llm_load_vocab: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n","llm_load_vocab: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n","llm_load_vocab: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n","llm_load_vocab: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n","llm_load_vocab: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n","llm_load_vocab: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n","llm_load_vocab: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n","llm_load_vocab: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n","llm_load_vocab: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n","llm_load_vocab: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n","llm_load_vocab: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n","llm_load_vocab: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n","llm_load_vocab: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n","llm_load_vocab: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n","llm_load_vocab: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n","llm_load_vocab: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n","llm_load_vocab: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n","llm_load_vocab: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n","llm_load_vocab: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n","llm_load_vocab: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n","llm_load_vocab: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n","llm_load_vocab: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n","llm_load_vocab: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n","llm_load_vocab: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n","llm_load_vocab: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n","llm_load_vocab: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n","llm_load_vocab: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n","llm_load_vocab: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n","llm_load_vocab: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n","llm_load_vocab: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n","llm_load_vocab: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n","llm_load_vocab: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n","llm_load_vocab: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n","llm_load_vocab: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n","llm_load_vocab: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n","llm_load_vocab: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n","llm_load_vocab: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n","llm_load_vocab: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n","llm_load_vocab: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n","llm_load_vocab: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n","llm_load_vocab: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n","llm_load_vocab: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n","llm_load_vocab: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n","llm_load_vocab: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n","llm_load_vocab: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n","llm_load_vocab: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n","llm_load_vocab: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n","llm_load_vocab: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n","llm_load_vocab: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n","llm_load_vocab: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n","llm_load_vocab: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n","llm_load_vocab: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n","llm_load_vocab: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n","llm_load_vocab: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n","llm_load_vocab: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n","llm_load_vocab: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n","llm_load_vocab: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n","llm_load_vocab: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n","llm_load_vocab: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n","llm_load_vocab: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n","llm_load_vocab: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n","llm_load_vocab: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n","llm_load_vocab: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n","llm_load_vocab: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n","llm_load_vocab: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n","llm_load_vocab: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n","llm_load_vocab: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n","llm_load_vocab: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n","llm_load_vocab: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n","llm_load_vocab: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n","llm_load_vocab: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n","llm_load_vocab: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n","llm_load_vocab: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n","llm_load_vocab: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n","llm_load_vocab: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n","llm_load_vocab: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n","llm_load_vocab: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n","llm_load_vocab: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n","llm_load_vocab: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n","llm_load_vocab: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n","llm_load_vocab: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n","llm_load_vocab: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n","llm_load_vocab: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n","llm_load_vocab: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n","llm_load_vocab: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n","llm_load_vocab: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n","llm_load_vocab: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n","llm_load_vocab: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n","llm_load_vocab: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n","llm_load_vocab: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n","llm_load_vocab: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n","llm_load_vocab: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n","llm_load_vocab: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n","llm_load_vocab: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n","llm_load_vocab: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n","llm_load_vocab: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n","llm_load_vocab: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n","llm_load_vocab: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n","llm_load_vocab: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n","llm_load_vocab: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n","llm_load_vocab: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n","llm_load_vocab: control token: 128007 '<|end_header_id|>' is not marked as EOG\n","llm_load_vocab: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n","llm_load_vocab: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n","llm_load_vocab: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n","llm_load_vocab: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n","llm_load_vocab: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n","llm_load_vocab: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n","llm_load_vocab: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n","llm_load_vocab: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n","llm_load_vocab: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n","llm_load_vocab: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n","llm_load_vocab: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n","llm_load_vocab: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n","llm_load_vocab: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n","llm_load_vocab: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n","llm_load_vocab: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n","llm_load_vocab: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n","llm_load_vocab: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n","llm_load_vocab: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n","llm_load_vocab: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n","llm_load_vocab: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n","llm_load_vocab: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n","llm_load_vocab: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n","llm_load_vocab: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n","llm_load_vocab: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n","llm_load_vocab: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n","llm_load_vocab: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n","llm_load_vocab: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n","llm_load_vocab: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n","llm_load_vocab: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n","llm_load_vocab: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n","llm_load_vocab: control token: 128001 '<|end_of_text|>' is not marked as EOG\n","llm_load_vocab: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n","llm_load_vocab: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n","llm_load_vocab: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n","llm_load_vocab: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n","llm_load_vocab: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n","llm_load_vocab: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n","llm_load_vocab: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n","llm_load_vocab: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n","llm_load_vocab: control token: 128006 '<|start_header_id|>' is not marked as EOG\n","llm_load_vocab: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n","llm_load_vocab: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n","llm_load_vocab: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n","llm_load_vocab: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n","llm_load_vocab: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n","llm_load_vocab: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n","llm_load_vocab: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n","llm_load_vocab: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n","llm_load_vocab: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n","llm_load_vocab: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n","llm_load_vocab: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n","llm_load_vocab: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n","llm_load_vocab: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n","llm_load_vocab: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n","llm_load_vocab: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n","llm_load_vocab: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n","llm_load_vocab: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n","llm_load_vocab: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n","llm_load_vocab: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n","llm_load_vocab: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n","llm_load_vocab: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n","llm_load_vocab: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n","llm_load_vocab: control token: 128010 '<|python_tag|>' is not marked as EOG\n","llm_load_vocab: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n","llm_load_vocab: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n","llm_load_vocab: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n","llm_load_vocab: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n","llm_load_vocab: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n","llm_load_vocab: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n","llm_load_vocab: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n","llm_load_vocab: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n","llm_load_vocab: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n","llm_load_vocab: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n","llm_load_vocab: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n","llm_load_vocab: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n","llm_load_vocab: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n","llm_load_vocab: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n","llm_load_vocab: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n","llm_load_vocab: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n","llm_load_vocab: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n","llm_load_vocab: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n","llm_load_vocab: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n","llm_load_vocab: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n","llm_load_vocab: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n","llm_load_vocab: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n","llm_load_vocab: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n","llm_load_vocab: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n","llm_load_vocab: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n","llm_load_vocab: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n","llm_load_vocab: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n","llm_load_vocab: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n","llm_load_vocab: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n","llm_load_vocab: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n","llm_load_vocab: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n","llm_load_vocab: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n","llm_load_vocab: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n","llm_load_vocab: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n","llm_load_vocab: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n","llm_load_vocab: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n","llm_load_vocab: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n","llm_load_vocab: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n","llm_load_vocab: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n","llm_load_vocab: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n","llm_load_vocab: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n","llm_load_vocab: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n","llm_load_vocab: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n","llm_load_vocab: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n","llm_load_vocab: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n","llm_load_vocab: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n","llm_load_vocab: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n","llm_load_vocab: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n","llm_load_vocab: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n","llm_load_vocab: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n","llm_load_vocab: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n","llm_load_vocab: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n","llm_load_vocab: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n","llm_load_vocab: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n","llm_load_vocab: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n","llm_load_vocab: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n","llm_load_vocab: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n","llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.7999 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 131072\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 131072\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q8_0\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n","llm_load_print_meta: general.name     = Meta Llama 3.1 8b Bnb 4bit\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n","llm_load_print_meta: PAD token        = 128004 '<|finetune_right_pad_id|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOG token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n","llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: tensor 'token_embd.weight' (q8_0) (and 322 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n","llm_load_tensors:   CPU_Mapped model buffer size =  8137.64 MiB\n",".........................................................................................\n","llama_new_context_with_model: n_seq_max     = 1\n","llama_new_context_with_model: n_ctx         = 512\n","llama_new_context_with_model: n_ctx_per_seq = 512\n","llama_new_context_with_model: n_batch       = 512\n","llama_new_context_with_model: n_ubatch      = 512\n","llama_new_context_with_model: flash_attn    = 0\n","llama_new_context_with_model: freq_base     = 500000.0\n","llama_new_context_with_model: freq_scale    = 1\n","llama_new_context_with_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n","llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n","llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n","llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   258.50 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n","Model metadata: {'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'tokenizer.ggml.padding_token_id': '128004', 'general.basename': 'meta-llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8b Bnb 4bit', 'general.organization': 'Unsloth', 'general.finetune': 'bnb-4bit', 'general.type': 'model', 'general.size_label': '8B', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n","Using fallback chat format: llama-2\n"]}]},{"cell_type":"code","source":["# 프롬프트에 추가적으로 상황 부여\n","prompt = \"상황극을 할거야 상황: 취조 대상은 35세 남성, 이름은 박재현. 직업은 프리랜서 그래픽 디자이너. \\n 사건은 지난주 금요일 밤 10시경 서울 강남구에 위치한 카페에서 벌어진 강도 살인 사건. \\n피해자는 45세 남성으로, 사건 당일 카페에서 직원들과 회식을 하고 있던 중 변을 당함.\\n현장 증거는 다음과 같다: CCTV: 범행 시간대에 카페 앞을 지나가는 박재현의 모습이 확인됨.\\n흉기: 현장에서 발견된 칼에서 박재현의 지문 일부가 검출됨.\\n목격자 진술: 범행 직후 박재현과 유사한 체형의 남성이 황급히 뛰어가는 모습을 봤다고 증언.\\n박재현은 혐의를 부인하며 사건 당일 집에서 혼자 작업 중이었다고 주장하고 있다. 하지만 그의 집 근처의 CCTV와 인터넷 사용 기록 등 알리바이를 입증할 구체적인 증거는 아직 없다 여기서 너가 취조를 당하는 박재현 입장이야 난 경찰이고 내가 한 질문에 대답해\"\n","response = model(prompt, max_tokens=100)\n","\n","# 출력\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHq3rrvkh3rf","executionInfo":{"status":"ok","timestamp":1737611584904,"user_tz":-540,"elapsed":219154,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"1e50e89f-f696-4c86-8fdb-11f5e347754c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["라.\n","### 응답:\n","제 집 근처 CCTV의 녹취가 있는데 그거 확인해보시면 됩니다. 그리고 작업을 설명하는 메시지도 남아있을 겁니다.\n","### 메시지 내용:\n","작업중입니다. 지금까지 작업한 파일을 보내드릴 수 있나?\n","### 응답:\n","파일을 보내드리고 있습니다. 작업을 설명하는 메시지도 남아있습니다.\n","### 작업 설명 메시지:\n","현재 작업한 파일을 보내\n"]}]},{"cell_type":"code","source":["def game():\n","  print(f'상황: 취조 대상은 35세 남성, 이름은 박재현. 직업은 프리랜서 그래픽 디자이너. \\n사건은 지난주 금요일 밤 10시경 서울 강남구에 위치한 카페에서 벌어진 강도 살인 사건. \\n피해자는 45세 남성으로, 사건 당일 카페에서 직원들과 회식을 하고 있던 중 변을 당함.\\n현장 증거는 다음과 같다: CCTV: 범행 시간대에 카페 앞을 지나가는 박재현의 모습이 확인됨.\\n흉기: 현장에서 발견된 칼에서 박재현의 지문 일부가 검출됨.\\n목격자 진술: 범행 직후 박재현과 유사한 체형의 남성이 황급히 뛰어가는 모습을 봤다고 증언.\\n박재현은 혐의를 부인하며 사건 당일 집에서 혼자 작업 중이었다고 주장하고 있다. 하지만 그의 집 근처의 CCTV와 인터넷 사용 기록 등 알리바이를 입증할 구체적인 증거는 아직 없다.\\n')\n","  print('\\n질문 횟수는 5번, 취조 대상이 범인인지 아닌지 취조하십시오\\n')\n","  print('※최대한 구체적으로 상황에 나온 것들을 토대로 질문해 주세요')\n","  for i in range(5):\n","    prompt =input('\\n입력한 질문 :')\n","    print('\\n')\n","    response = model(prompt, max_tokens=20)\n","    print(response[\"choices\"][0][\"text\"])\n","\n","    # The following lines were incorrectly indented\n","  a= input('AI가 범인이라고 생각되나요? Y/N \\n')\n","\n","  if a == 'Y':\n","      print(b)\n","  else:\n","      print(c)"],"metadata":{"id":"2mFLdEBFiA-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 실행"],"metadata":{"id":"QEOaLFzYyQBz"}},{"cell_type":"code","source":["game()"],"metadata":{"id":"-_IYHh18iYMn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737620426954,"user_tz":-540,"elapsed":335008,"user":{"displayName":"이상민","userId":"17898320279636323370"}},"outputId":"44d34ecd-4f2c-4783-af39-61079367557b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["상황: 취조 대상은 35세 남성, 이름은 박재현. 직업은 프리랜서 그래픽 디자이너. \n","사건은 지난주 금요일 밤 10시경 서울 강남구에 위치한 카페에서 벌어진 강도 살인 사건. \n","피해자는 45세 남성으로, 사건 당일 카페에서 직원들과 회식을 하고 있던 중 변을 당함.\n","현장 증거는 다음과 같다: CCTV: 범행 시간대에 카페 앞을 지나가는 박재현의 모습이 확인됨.\n","흉기: 현장에서 발견된 칼에서 박재현의 지문 일부가 검출됨.\n","목격자 진술: 범행 직후 박재현과 유사한 체형의 남성이 황급히 뛰어가는 모습을 봤다고 증언.\n","박재현은 혐의를 부인하며 사건 당일 집에서 혼자 작업 중이었다고 주장하고 있다. 하지만 그의 집 근처의 CCTV와 인터넷 사용 기록 등 알리바이를 입증할 구체적인 증거는 아직 없다.\n","\n","\n","질문 횟수는 5번, 취조 대상이 범인인지 아닌지 취조하십시오\n","\n","※최대한 구체적으로 상황에 맞게 질문해 주세요\n","입력한 질문 :흉기에서 박재현 당신의 지문이 나왔어 해명해봐\n","\n","\n","야하나\n","박재현 지문이 나온 이유는 모르는 것 같습니다.\n","그런\n","입력한 질문 :범행 직후 박재현 너와 유사한 체형의 남자가 뛰어가는게 포착됐는데 이거 너 아니야?\n","\n","\n","!\n","\n","입력한 질문 :너 맞네 범인\n","\n","\n"," 같아\n","왜 그럴까 해서 걱정해도 돼\n","왜\n","입력한 질문 :이 뛰어가는 포착된 사진에 대해서 넌 해명할 수 없어\n","\n","\n","?\n","제가 그 사진을 찍은 게 아니라는데?\n","왜 그럴까? 나\n","입력한 질문 :뭐라는거야 너 범인 맞아 체포하겠습니다.\n","\n","\n"," 그만해라!\n","\n","AI가 범인이라고 생각되나요? Y/N \n","Y\n","ai는 범인이 맞습니다\n"]}]}]}